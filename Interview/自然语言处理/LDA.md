# 详述LDA原理？
- 从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)
    - 多项式分布的共轭分布是狄利克雷分布
    - 二项式分布的共轭分布是Beta分布
- 从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)
- 从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)
- 从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg)
- 文档里某个单词出现的概率可以用公式表示：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b9y6avdtj306e01jdfo.jpg)
- 采用EM方法修正词-主题矩阵+主题-文档矩阵直至收敛

# LDA中的主题矩阵如何计算?词分布矩阵如何计算？
这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点
- 吉布斯采样
	- 先随机给每个词附上主题
	- 因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布
	- 有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布
	- 根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题
	- 收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布

- 通常会引申出如下几个问题：
	- 吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）
	- MCMC中什么叫做蒙特卡洛方法？
		- 通常用于求概率密度的积分
		- 用已知分布去评估未知分布
		- reject-acpect过程
	- 马尔科夫链收敛性质？
		- 非周期性，不能出现死循环
		- 连通性，不能有断点
	- MCMC中什么叫做马尔科夫链采样过程？
		- 先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵
		- 再根据平稳矩阵后的条件概率p(x/xt)得到平稳分布的样本集(xn+1,xn+2...)
	- 给定平稳矩阵如何得到概率分布样本集？
		- M-C采样
			- 给定任意的转移矩阵Q，已知π(i)p(i,j) = π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j) = π(j)Q(j,i)a(j,i)
			- 根据Q的条件概率Q(x/xt)得到xt+1
			- u~uniform
			- u<π(xt+1)Q(xt+1,xt) 则accept，就和蒙特模拟一样否则xt+1 = xt
			- (xt,xt+1...)代表着我们的分布样本集
		- M-H采样
			- 左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度
		- Gibbs采样
			- 同上，差别在固定n−1个特征在某一个特征采样及坐标轮换采样
	- 什么叫做坐标转换采样？
		- 平面上任意两点满足细致平稳条件π(A)P(A->B) = π(B)P(B->A)
		- 从条件概率分布P(x2|x(t)1)中采样得到样本x(t+1)2
		- 从条件概率分布P(x1|x(t+1)2)中采样得到样本x(t+1)1
		- 其为一对样本，有点像Lasso回归中的固定n-1维特征求一维特征求极值的思路

- 变分推断EM算法
	- 整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta
	- 变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**
		- 实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵
	- EM过程
		- E：最小化相对熵，偏导为0得到变分参数
		- M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值

# LDA的共轭分布解释下?
以多项式分布-狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布


# PLSA和LDA的区别?
- LDA是加了狄利克雷先验的PLSA
- PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的
- LDA是贝叶斯思想，PLSA是MLE

# 怎么确定LDA的topic个数
- 对文档d属于哪个topic有多不确定，这个不确定程度就是Perplexity
- 多次尝试，调优perplexity-topic number曲线
    - 困惑度越小，越容易过拟合
    - 某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg)

# LDA和Word2Vec区别？LDA和Doc2Vec区别？
- LDA比较是doc，word2vec是词
- LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示
    - LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息
- LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果

# LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?
- 通常alpha为1/k，k为类别数，beta一般为0.01
- alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确
- beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度
- chucksize大一些更新的过程比较平稳，收敛更加平稳
- 迭代次数一般不超过2000次，200万doc大约在2300次收敛